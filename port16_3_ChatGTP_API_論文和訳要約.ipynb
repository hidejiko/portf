{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI ChatGPT APIを使って安価にPDF論文を和訳・要約"
      ],
      "metadata": {
        "id": "M4B9FbRwb93V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "c_E7hEbDcPNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e2735a-fcf0-42a2-d65c-68f0cf8da435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/220.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVkASziWNUzh",
        "outputId": "bf62eb6b-e89a-42a1-c052-ca6f454e05a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import re\n",
        "import numpy as np\n",
        "import yaml\n",
        "\n",
        "import openai\n",
        "from pdfminer.high_level import extract_text"
      ],
      "metadata": {
        "id": "CET57-t6bxWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAIのAPIキーを設定(1)\n",
        "#openai.api_key = \"\""
      ],
      "metadata": {
        "id": "GI2DUQfZOUyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAIのAPIキーを設定(2)\n",
        "\n",
        "f = open('gptkey.txt','r')\n",
        "ai_key = f.read()\n",
        "f.close()\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key = ai_key\n",
        ")\n"
      ],
      "metadata": {
        "id": "KwdWpa9QOUSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvyG8X-Lbjcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5703bc0-55c5-4f25-a24b-ae83972ee01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input file has 9396 words, so API requests repeat 6.0 times.\n",
            "0 times request... (1792 words are included)\n",
            "- 論文のタイトルは「なぜ私はあなたを信頼すべきか？」で、予測の説明方法について述べています。\n",
            "- この研究では、予測の理由を説明することが信頼性の評価に非常に重要であり、新しいモデルを展開する際にも重要であることを示しています。\n",
            "- 著者たちは、LIMEという新しい説明技術を提案しており、予測の周りで局所的に解釈可能なモデルを学習することで、任意の分類器の予測を説明する方法を示しています。\n",
            "- また、代表的な個別の予測とその説明を非冗長な方法で提示することにより、モデルの説明方法を提案しています。\n",
            "- さまざまなテキスト（ランダムフォレストなど）や画像分類（ニューラルネットワークなど）のための異なるモデルを説明する柔軟性を示しています。\n",
            "- 新しい実験を通じて、説明の有用性を示しており、予測を信頼すべきかどうかを判断する、モデルを選択する、信頼性の低い分類器を改善する、なぜ分類器を信頼すべきでないかを特定するなど、さまざまなシナリオでの結果を示しています。\n",
            "1 times request... (1792 words are included)\n",
            "- LIMEは、モデルに対してローカルに忠実でありながら解釈可能な説明を提供するための手法である。\n",
            "- 解釈可能なデータ表現は、実際のモデルが使用する特徴とは異なる人間に理解可能な表現である。\n",
            "- LIMEは、近傍のインスタンスをサンプリングし、それらに対してモデルの予測を行い、重み付けすることで、ローカルな振る舞いを学習する。\n",
            "- LIMEは、疎な線形モデルを用いて説明を生成するが、他のモデルや複雑なモデルにも適用可能である。\n",
            "- LIMEの具体的な例として、テキスト分類と画像分類の説明が示されている。\n",
            "2 times request... (1792 words are included)\n",
            "- アルゴリズム1は、個々の予測の説明を生成するため、データセットのサイズには依存せず、f（x）の計算時間とサンプルの数Nに依存します。\n",
            "- ランダムフォレストの説明（scikit-learnを使用）は、N = 5000のラップトップで約3秒かかります。\n",
            "- インセプションネットワークの各予測の説明には約10分かかります。\n",
            "- 解釈可能な表現とGの選択には欠点があります。モデルがブラックボックスとして扱える一方、特定の解釈可能な表現では特定の振る舞いを説明するのに十分な力がありません。\n",
            "- サブモジュラーピックは、個別の予測の説明に加えて、モデル全体の理解を提供します。説明の選択は、ユーザーが見るために選んだB個のインスタンスを選ぶというタスクです。\n",
            "- サブモジュラーピックは、特徴の重要性を考慮して、重要なインスタンスを選択します。\n",
            "- シミュレートされたユーザーエクスペリメントでは、説明の有用性を評価します。説明がモデルに忠実であるか、予測の信頼性を評価するのに役立つか、モデル全体を評価するのに役立つかを調査します。\n",
            "3 times request... (1792 words are included)\n",
            "- 研究では、異なる説明手法による信頼性の平均F1スコアを比較した結果を示しています。\n",
            "- LIMEが他の手法に比べて信頼性が高いことが示されています。\n",
            "- 実験では、説明を使用してモデルの選択を行うことができるかどうかを評価しました。\n",
            "- 実験結果では、説明を使用することでモデルの選択が改善されることが示されています。\n",
            "- また、説明を使用することでモデルの特徴量エンジニアリングが可能であり、モデルのパフォーマンスが向上することが確認されました。\n",
            "- 最後に、説明を使用してモデルの異常を特定することができるかどうかを評価しました。結果は成功しました。\n",
            "\n",
            "注意: この要約は元の論文の一部を要約したものであり、情報の正確性を保証するものではありません。詳細な情報が必要な場合は、元の論文を参照してください。\n",
            "4 times request... (1792 words are included)\n",
            "- テストセットの正確性では、誤った分類器の選択結果となる可能性があるため、どの分類器を信頼するかを判断するのに有用である。\n",
            "- サブモジュラーピック（SP）は、ランダムピック（RP）と比較して、ユーザーが最良の分類器を選択する能力を大幅に向上させる。\n",
            "- 説明は、特にユーザーが一般化しないと感じる特徴を除去するために、特徴工学を支援することができる。\n",
            "- LIMEを使用して、専門家でなくても、説明が存在する場合にこれらの異常を識別できることを示した。\n",
            "- LIMEは、既存のシステムを補完し、予測が「正しい」と思われるが間違った理由で行われた場合でも、ユーザーが信頼を評価できるようにすることができる。\n",
            "- LIMEは、任意の分類器や回帰器に適したドメインに適用できる一般的なモデルに対して解釈可能な説明を提供する。\n",
            "- LIMEは、オリジナルモデルの予測を学習することでモデルに依存しない説明を学習する一般的なフレームワークである。\n",
            "- LIMEは、説明を最適化し、ローカルでオリジナルモデルに近似するモデルを見つけるというより実現可能なタスクを解決する。\n",
            "- LIMEは、認知上の制約を明示的に考慮せずに行われる他のアプローチとは異なり、解釈可能な説明を生成する。\n",
            "- LIMEの将来の研究課題として、さまざまな説明ファミリーの比較的研究、画像への適用方法の検討、他のドメインへの応用の探索、理論的な特性や計算上の最適化の研究などが挙げられる。\n",
            "5 times request... (436 words are included)\n",
            "- 画像の説明に関する研究が行われており、その中でいくつかの関連研究が紹介されている。\n",
            "- データマイニングにおける情報漏洩についての研究が存在し、その検出と回避方法が提案されている。\n",
            "- 部分集合関数の最大化に関する研究が行われており、その応用が紹介されている。\n",
            "- 説明的なデバッグの原則に基づいた機械学習の個別化に関する研究が行われている。\n",
            "- 規則とベイズ分析を使用した解釈可能な分類器の構築に関する研究が行われている。\n",
            "- データ駆動型のドキュメント分類に関する研究が存在し、その解釈方法が提案されている。\n",
            "- 単語と句の分散表現に関する研究が行われている。\n",
            "- 機械学習の実装と分析のための統合サポートに関する研究が行われている。\n",
            "- ソフトウェア開発のツールとして統計的な機械学習を調査する研究が行われている。\n",
            "- 潜在変数モデルの正確で記述的な表現の抽出に向けた研究が行われている。\n",
            "- 機械学習システムの隠れた技術的負債に関する研究が存在し、その影響が議論されている。\n",
            "- ゲーム理論を使用した個々の分類の効率的な説明に関する研究が行われている。\n",
            "- 畳み込みニューラルネットワークの改良に関する研究が存在し、その成果が紹介されている。\n",
            "- 最適化された医療スコアリングシステムのための超疎な線形整数モデルに関する研究が行われている。\n",
            "- ルールリストの研究が行われており、その有用性が議論されている。\n",
            "- ニューラルネットワークによる画像のキャプション生成に関する研究が行われている。\n",
            "- ビジョンシステムの故障を予測する研究が存在し、その手法が提案されている。\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# APIへ送信するワード数の上限を、経験的に設定する\n",
        "WC_MAX = int(4096 * 7 / 16)\n",
        "\n",
        "# PDFファイルからテキスト情報を抽出する\n",
        "def extract_text_from_pdf(file_path) :\n",
        "    text = extract_text(file_path)\n",
        "    return text\n",
        "\n",
        "# テキスト情報から単語途中で現れるハイフンと改行を削除する\n",
        "def clean_extracted_text(text) :\n",
        "    text = text.replace('-\\n', '')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# APIへ送信するメッセージ（プロンプト）を作成する\n",
        "def create_prompt(text, total_num_req) :\n",
        "    return f\"\"\"英語の研究論文の一部を日本語で要約するタスクを行います。\n",
        "研究論文は全部で{total_num_req + 1}個に分割しています。\n",
        "以下のルールに従ってください。\n",
        "\n",
        "・リスト形式で出力する (先頭は - を使う)\n",
        "・簡潔に表現する\n",
        "・不明な単語や人名と思われるものは英語のまま表示する\n",
        "\n",
        "それでは開始します。\n",
        "\n",
        "英語の論文の一部:\n",
        "{text}\n",
        "\n",
        "日本語で要約した文章:\"\"\"\n",
        "\n",
        "# テキストをWC_MAXごとに分割する\n",
        "def split_text(text, wc_max) :\n",
        "    '''\n",
        "    :param text: english oneline sentence\n",
        "    :param wc_max:  word count max (if token max is 4097, it should be 2048)\n",
        "    :return: chunk[]\n",
        "    '''\n",
        "    words = text.split()  # Split the text into words\n",
        "    chunks = [' '.join(words[i :i + wc_max]) for i in range(0, len(words), wc_max)]\n",
        "    return chunks\n",
        "\n",
        "# main()メソッド\n",
        "def main() :\n",
        "    # PDFファイルからテキスト情報を抽出して整形する\n",
        "    extracted_text = extract_text_from_pdf('/content/2939672.2939778.pdf')\n",
        "    clean_text = clean_extracted_text(extracted_text)\n",
        "\n",
        "    # テキスト情報の概要を表示する。APIコール回数を表示する。\n",
        "    print(f\"input file has {len(clean_text.split())} words, \"\n",
        "          f\"so API requests repeat {np.ceil(len(clean_text.split()) / WC_MAX)} times.\")\n",
        "\n",
        "    # 分割処理（4,097トークン以下に抑えるための処理）\n",
        "    splited_clean_text = split_text(clean_text, WC_MAX)\n",
        "\n",
        "    # 分割したテキストごとに要約をリクエストする\n",
        "    for i in range(len(splited_clean_text)) :\n",
        "        print(f\"{i} times request... ({len(splited_clean_text[i].split())} words are included)\")\n",
        "        # Setup prompt messages\n",
        "        messages = [\n",
        "            {\"role\" : \"system\",\n",
        "             \"content\" : \"あなたは、金融機関に所属するサイバーセキュリティ研究者です。あなたは、前後に問い合わせした内容を考慮して思慮深い回答をします。\"},\n",
        "            {\"role\" : \"user\", \"content\" : create_prompt(splited_clean_text[i], len(splited_clean_text))}\n",
        "        ]\n",
        "\n",
        "        # APIにリクエストを送信する\n",
        "        # token数超過などの例外を返す可能性があるため、try/exceptを設定しておく。\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",  # GPTのモデルを指定。安価なgpt-3.5-turbo\n",
        "                messages=messages,\n",
        "                max_tokens=800,  # 生成するトークンの最大数（入力と合算して4097以内に収める必要あり）\n",
        "                n=1,  # 生成するレスポンスの数\n",
        "                stop=None,  # 停止トークンの設定\n",
        "                temperature=0.7,  # 生成時のランダム性の制御\n",
        "                top_p=1,  # トークン選択時の確率閾値\n",
        "            )\n",
        "            # 生成するレスポンス数は1という前提\n",
        "            print(response.choices[0].message.content)\n",
        "        except openai.error.InvalidRequestError as e:\n",
        "            print(\"OpenAI API error occurred: \" + str(e))\n",
        "main()\n",
        "\n"
      ]
    }
  ]
}